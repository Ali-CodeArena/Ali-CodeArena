<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8" />
  <meta name="description" content="CodeArena" />
  <meta name="keywords" content="Multilingual, Code, Large Language Models, LLM, Code LLM, Evaluation, Benchmark" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>
    CodeArena
  </title>
  <link href="https://fonts.googleapis.com/css2?family=IM+Fell+Great+Primer:ital@0;1&display=swap" rel="stylesheet" />
  <link href="https://fonts.googleapis.com/css2?family=Cinzel:wght@400;700&display=swap" rel="stylesheet"></link>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <!-- <script
      async
      src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"
    ></script>
    <script>
      window.dataLayer = window.dataLayer || []

      function gtag() {
        dataLayer.push(arguments)
      }

      gtag("js", new Date())

      gtag("config", "G-PYVRSFMDRL")
    </script> -->

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet" />

  <link rel="stylesheet" href="./css/bulma.min.css" />
  <link rel="stylesheet" href="./css/bulma-carousel.min.css" />
  <link rel="stylesheet" href="./css/bulma-slider.min.css" />
  <link rel="stylesheet" href="./css/fontawesome.all.min.css" />
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css" />
  <link rel="stylesheet" href="./css/index.css" />
  <link rel="icon" href="./images/favicon.svg" />

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./js/fontawesome.all.min.js"></script>
  <script src="./js/bulma-carousel.min.js"></script>
  <script src="./js/bulma-slider.min.js"></script>
  <script src="./js/index.js"></script>
</head>

<body>

  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">
              Evaluating and Aligning CodeLLMs on Human Preference
            </h1>
        <div class="is-size-5 publication-authors">
              <span class="author-block">
                <a href="">Jian Yang</a><sup>1</sup>,</span>
              <span class="author-block">
                <a href="">Jiaxi Yang</a><sup>2,3</sup>,</span>
              <span class="author-block">
                <a href="">Ke Jin</a><sup></sup>,</span>
              <span class="author-block">
                <a href="">Yibo Miao</a><sup>4</sup>,</span>
              <span class="author-block">
                <a href="">Lei Zhang</a><sup>2,3</sup>,</span>
              <span class="author-block">
                <a href="">Liqun Yang</a><sup></sup>,</span>
              <span class="author-block">
                <a href="">Zeyu Cui</a><sup>1</sup>,</span>
              <span class="author-block">
                <a href="">Yichang Zhang</a><sup>1</sup>,</span>
              <span class="author-block">
                <a href="">Binyuan Hui</a><sup>1</sup>,</span>
              <span class="author-block">
                <a href="">Junyang Lin</a><sup>1</sup>,</span>
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block"><sup>1</sup>Alibaba Group;</span>
              <span class="author-block"><sup>2</sup>Shenzhen Institute of Advanced Technology, Chinese Academy of Sciences;</span>
              <span class="author-block"><sup>3</sup>University of Chinese Academy of Sciences;</span>
              <span class="author-block"><sup>4</sup>Shanghai Jiao Tong University;</span>
            </div>

            <div class="column has-text-centered">
              <div class="publication-links">
                <!-- PDF Link. -->
                <span class="link-block">
                  <a href="" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                    </span>
                    <span>Paper</span>
                  </a>
                </span>
                <span class="link-block">
                  <a href=""
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>
                <!-- Dataset Link. -->
                <span class="link-block">
                  <a href=""
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="far fa-images"></i>
                    </span>
                    <span>Evaluation Data</span>
                  </a>
                </span>
                <!-- Leaderboard Link. -->
                <span class="link-block">
                  <a href="leaderboard.html" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fa fa-trophy"></i>
                    </span>
                    <span>Leaderboard</span>
                  </a>
                </span>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="hero teaser">
    <div class="container is-max-desktop">
      <div class="hero-body">
        <div class="columns is-centered">
          <!-- center the image -->
          <img src="./images/categories_sunburst.pdf" alt="Teaser" class="teaser-image center" width="60%" />
        </div>
      </div>
    </div>
  </section>


  <section class="section">
    <div class="container is-max-desktop">
      <!-- Abstract. -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified">
            <p>
              Code large language models (codeLLMs) have
made significant strides in code generation.
Most previous code-related benchmarks, which
consist of various programming exercises along
with the corresponding test cases, are used
as a common measure to evaluate the performance and capabilities of code LLMs. How-
ever, the current codeLLMs focus on synthesizing the correct code snippet, ignoring the
alignment with human preferences, where the
query should sampled from the practical application scenarios and the model-generated
responses should satisfy the human preference. To bridge the gap between the modelgenerated response and human preference, we
present a rigorous human-curated benchmark
CodeArena to emulate the complexity and diversity of real-world coding tasks, where 397
high-quality samples spanning 40 categories,
carefully curated from user queries. Further, we
propose a diverse synthetic instruction corpus
SynCode-Instruct (nearly 10B tokens) by scaling instructions from the website. The results
find performance differences between code execution-based benchmarks and CodeArena.
Our systematic experiments of CodeArena on
20+ LLMs reveal a notable performance gap between open codeLLMs (e.g. Qwen-Coder) and
closed-source LLMs (e.g., o1 and Claude series), underscoring the importance of the alignment of the human preference.
            </p>
          </div>
        </div>
      </div>
      <!--/ Abstract. -->
    </div>
  </section>
  <!--
  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Contamination</h2>
          <div class="content has-text-justified">
            <p>
              <span class="dnerf">McEval</span> annotates problems with release dates, and thus allows evaluating
              models on problems released during a specific time period. Thus, for a newer model
              with a training-cutoff date <span class="dnerf">D</span>, we can evaluate it on problems released after
              <span class="dnerf">D</span> to measure its generalization on <i>unseen</i> problems.
            </p>
            <!-- side by side images->
            <div class="columns is-centered">
              <img src="./images/contamination1.png" alt="Code Generation Live Evaluation" class="teaser-image"
                width="48%" class="center" />
              <img src="./images/contamination2.png" alt="Test Output Prediction Live Evaluation" class="teaser-image"
                width="48%" class="center" />
            </div>

            <p>
              The above plots depict the performance of models on code generation and test output prediction scenarios
              on problems released over different months. We find that <span class="dnerf">DeepSeek</span> models
              exhibit a stark drop in performance on LeetCode problems released since September 2023, its release date,
              indicating that the earlier problems might be contaminated. In contrast, for
              <span class="dnerf">GPT</span>
              models, the performance is relatively stable across different months.
            </p>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Holistic Evaluation and Open vs Closed Models</h2>
          <div class="content has-text-justified">
            <p>
              <span class="dnerf">McEval</span> evaluates models on a variety of code-related scenarios, such as
              code generation, self-repair, test output prediction, and code execution. We find that while model
              performances
              are correlated across different scenarios, there relative performances and ordering can vary (left
              figure).
              For instance, <span class="dnerf">Claude-3-Opus</span> overtakes <span class="dnerf">GPT-4-turbo</span> in
              the
              test output prediction scenario, but not in the code generation scenario. Similarly,
              <span class="dnerf">Mistral-Large</span>
              performs considerably better on natural language reasoning tasks like test output prediction and code
              execution.
            </p>
            <br />
            <div class="columns is-centered">
              <img src="./images/tasks_radar.png" alt="Holistic Evaluation" class="teaser-image" width="44%"
                height="44%" class="center" />
              <img src="./images/lc_barchart.png" alt="Open vesus Closed Models" class="teaser-image" width="48%"
                class="center" />

              <!-- <img src="./images/lcb_vs_he.png" alt="HumanEval Overfitting" class="teaser-image" width="55%"
                height="55%" class="center" /> ->
            </div>
            <p>
              We compare the performance of open access models with closed api-access models on <span
                class="dnerf">McEval</span> and find that generally the closed api-access models outperform the
              open models. Particularly, the only open models that surpass the barrier are fine-tuned variants of large
              (30+B parameter) models.
            </p>

          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Potential Overfitting in HumanEval</h2>
          <div class="content has-text-justified">
            <div class="columns is-centered">
              <img src="./images/lcb_vs_he.png" alt="HumanEval Overfitting" class="teaser-image center" width="80%"
                height="80%" />
            </div>
            <p>
              We also find that models that perform well on <span class="dnerf">HumanEval</span> might be overfitting on
              the benchmark. Particularly, the models are separated into two clusters depicted
              by the green and red shaded region in the right scatterplot. The models in the green region perform
              similarly on
              <span class="dnerf">HumanEval</span> and <span class="dnerf">LCB-Easy</span>, while the models in the red
              region
              perform well on <span class="dnerf">HumanEval</span> but lag behind on <span
                class="dnerf">LCB-Easy</span>.
              For instance, <span class="dnerf">DS-Ins-1.3B</span> model outperforms <span
                class="dnerf">Gemini-Pro</span> and <span class="dnerf">Claude-Ins-1</span> but performs considerably
              worse on <span class="dnerf">LCB-Easy</span>.
              Interestingly, the models in the red region are mostly fine-tuned variants of open access models. On the
              other hand, base models and most of the closed api-access models lie in the green region. This highlights
              a potential lack of diverse fine-tuning data being employed by the open source community and the need for
              optimizing models for a broader set of code-related tasks.
            </p>
          </div>
        </div>
      </div>

    </div>
  </section>

  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Model Comparisions</h2>
          <div class="content has-text-justified">
            <div class="columns is-centered">
              <img src="./images/codegen_performance.png" alt="Code Generation Performance" class="teaser-image"
                width="48%" class="center" />
              <img src="./images/repair_performance.png" alt="Code Execution Performance" class="teaser-image"
                width="48%" class="center" />
            </div>
            <div class="columns is-centered">
              <img src="./images/testgen_performance.png" alt="Code Execution Performance" class="teaser-image"
                width="48%" class="center" />
              <img src="./images/execution_performance.png" alt="Code Execution Performance" class="teaser-image"
                width="48%" class="center" />
            </div>
            <p>
              The above plots depict the performance of models on different scenarios considered in <span
                class="dnerf">McEval</span>. We find that <span class="dnerf">GPT-4-turbo</span> and <span
                class="dnerf">Claude-3-Opus</span> models perform best across different scenarios. Among open source
              models, <span class="dnerf">DS-Ins-33B</span> and <span class="dnerf">Phind-34B</span> perform the best.
            </p>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Submitting Custom Models</h2>
          <div class="content has-text-justified">
            <p>
              To submit models to the leaderboard, you can run the evaluation using the evaluation scripts in
              <a href="https://github.com/McEval/McEval">GitHub</a>. Once you have the results,
              you can fill out <a href="https://forms.gle/h2abvAHh6UnhWzzd9">this form</a>. You will need to fill out
              model details and provide the generated evaluation file with model generations and pass@1 scores. We will
              review the submission and add the model to the leaderboard accordingly.
            </p>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Potential Overfitting in HumanEval</h2>
          <div class="content has-text-justified">
            <div class="columns is-centered">
              <img src="./images/lcb_vs_he.png" alt="HumanEval Overfitting" class="teaser-image center" width="80%"
                height="80%" />
            </div>
            <p>
              We also find that models that perform well on <span class="dnerf">HumanEval</span> might be overfitting on
              the benchmark. Particularly, the models are separated into two clusters depicted
              by the green and red shaded region in the right scatterplot. The models in the green region perform
              similarly on
              <span class="dnerf">HumanEval</span> and <span class="dnerf">LCB-Easy</span>, while the models in the red
              region
              perform well on <span class="dnerf">HumanEval</span> but lag behind on <span
                class="dnerf">LCB-Easy</span>.
              For instance, <span class="dnerf">DS-Ins-1.3B</span> model outperforms <span
                class="dnerf">Gemini-Pro</span> and <span class="dnerf">Claude-Ins-1</span> but performs considerably
              worse on <span class="dnerf">LCB-Easy</span>.
              Interestingly, the models in the red region are mostly fine-tuned variants of open access models. On the
              other hand, base models and most of the closed api-access models lie in the green region. This highlights
              a potential lack of diverse fine-tuning data being employed by the open source community and the need for
              optimizing models for a broader set of code-related tasks.
            </p>
          </div>
        </div>
      </div>
  </section>
  -->
  <!-- <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">CodeArena</h2>
          <div class="content has-text-justified">
            <div class="columns is-centered">
              <img src="./images/1_statistics.png" alt="HumanEval Overfitting" class="teaser-image center" width="80%"
                height="80%" />
            </div>
            <p>
              
            </p>
          </div>
        </div>
      </div>
  </section>
  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3"> Human Annotation & Quality Control</h2>
          <div class="content has-text-justified">
            <div class="columns is-centered">
              <img src="./images/2_bench_cases.png" alt="HumanEval Overfitting" class="teaser-image center" width="80%"
                height="80%" />
            </div>
            <p>
              <!-- To create the massively multilingual code evaluation benchmark McEval, the annotation of multilingual
              code samples is conducted utilizing a comprehensive and systematic human annotation procedure,
              underpinned by rigorously defined guidelines to ensure accuracy and consistency.
              Following a detailed training session on the annotation protocol, which emphasizes the importance of context,
              syntactical correctness, and semantic fidelity across languages, annotators are tasked with creating problem definitions and
              the corresponding solution. -->
            <!-- </p>
          </div>
        </div>
      </div>  
  </section>
<section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">mCoder</h2>
          <div class="content has-text-justified">
            <div class="columns is-centered">
              <img src="./images/3_framework.png" alt="HumanEval Overfitting" class="teaser-image center" width="80%"
                height="80%" />
            </div>
            <p>
              
            </p>
          </div>
        </div>
      </div>
</section>
<section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Further Analysis</h2>
          <div class="content has-text-justified">
            <div class="columns is-centered">
              <img src="./images/4_code_class.png" alt="HumanEval Overfitting" class="teaser-image center" width="80%"
                height="80%" />
            </div>
            <p>
              We categorize the programming languages of McEval into 5 programming paradigms and 11 application scenarios and summarize the performance of code LLMs on the code generation task. 
            </p>
          </div>
        </div>
      </div>
</section>
<section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Model Performance in Code Completion Tasks</h2>
          <div class="content has-text-justified">
            <div class="columns is-centered">
              <img src="./images/5_radar_class_result.png" alt="HumanEval Overfitting" class="teaser-image center" width="80%"
                height="80%" />
            </div>
            <p>
              It can be observed that code LLMs generally perform better in object-oriented and multi-paradigm
              programming languages (high-resource languages), while perform worse in functional and procedural programming languages (low-resource languages).
              In areas like web development and scientific computing, the gap between open-source and closed-source models is narrowing. However, for application scenarios, there is still a substantial gap
              between open-source models and the closed-source GPT-4 series in low-resource languages related to scripting, mobile development, and educational research. mCoder performs superior over multiple same-size models and even some larger open-source models.
            </p>
          </div>
        </div>
      </div>
</section>
<section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Unbalance in Different Languages</h2>
          <div class="content has-text-justified">
            <div class="columns is-centered">
              <img src="./images/6_cross_dataset.png" alt="HumanEval Overfitting" class="teaser-image center" width="80%"
                height="80%" />
            </div>
            <p>
              We compare the results of several open-source models on the Multipl-E multilingual benchmark with corresponding languages on McEval.
              We obtained scores for 11 programming languages (including Python, Java, JavaScript, C++, PHP, Rust, Swift, R, Lua, Racket, and Julia) from the BigCode
              leaderboard. As shown in Figure (1), due to the simplicity of Python language tasks in this dataset, many models exhibit significant score discrepancies between the
              two benchmarks. By examining Figure (2) and (3), it becomes evident that all models demonstrate
              consistent multilingual capabilities between Multipl-E and McEval. However, Figure (2) highlights
              a majority of models within the blue circle, indicating that the current state-of-the-art performance
              of most models primarily lies in high-resource languages like Python, while their proficiency in
              low-resource languages awaits further exploration and enhancement.
            </p>
          </div>
        </div>
      </div>
</section>
<section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Cross-lingual Transfer</h2>
          <div class="content has-text-justified">
            <div class="columns is-centered">
              <img src="./images/7_cross-lingual-transfer.png" alt="HumanEval Overfitting" class="teaser-image center" width="80%"
                height="80%" />
            </div>
            <p>
              We fine-tune the CodeQwen-1.5 model using Python-only data in McEval-Instruct
              and compare it with mCoder. CodeQwen-1.5 performs well in most high-resource languages, but CodeQwen without alignment exhibits unsatisfactory results in
              some low-resource languages due to the inability to follow instructions. As such, with fine-tuning using only Python data,
              CodeQwen-1.5-Python improves significantly across most languages. It shows that the CodeQwen foundation model already possesses strong coding capabilities
              but lacks adequate instruction-following skills. Therefore, fine-tuning with Python-only data can still effectively transfer instruction-following abilities to other languages,
              resulting in superior multilingual performance.
            </p>
          </div>
        </div>
      </div>
</section> --> -->
<!-- <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">CodeQwen-1.5-Chat performance on McEval for problems of different difficulty levels.</h2>
          <div class="content has-text-justified">
            <div class="columns is-centered">
              <img src="./images/8_level_statistics.png" alt="HumanEval Overfitting" class="teaser-image center" width="80%"
                height="80%" />
            </div>
            <p>
              Based on algorithmic complexity, we classify McEval into three levels (Easy/Medium/Hard).
In Figure 1, we conduct a statistical analysis of CodeQwen-1.5-Chat's performance on code generation tasks
across various languages. For most languages, the codeLLM can answer the majority of easy questions but struggles with medium and hard ones.
            </p>
          </div>
        </div>
      </div>
</section> -->
<!-- <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Examples of Multilingual Generation</h2>
          <div class="content has-text-justified">
            <div class="columns is-centered">
              <img src="./images/9_bench_completion_cases.png" alt="HumanEval Overfitting" class="teaser-image center" width="80%"
                height="80%" />
            </div>
            <p>
              The data mainly consists of an instruction part
              (including function name, function description, and function call cases),a reference solution, and
              a test cases part. Left Figure: an example of the Lisp language. Middle Figure: a file processing
              programming task in AWK language. During the evaluation, the corresponding file processing result
              by the generated code will be compared with the reference answer. Right Figure: an example of the R language.
            </p>
          </div>
        </div>
      </div>
</section> -->
<!-- <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Examples of Multilingual Explanation</h2>
          <div class="content has-text-justified">
            <div class="columns is-centered">
              <img src="./images/10_bench_explain_cases.png" alt="HumanEval Overfitting" class="teaser-image center" width="80%"
                height="80%" />
            </div>
            <p>
              The data mainly consists of an instruction part (including a complete function),
              a reference Explanation. Left Figure: an example of the Kotlin language. Middle Figure: an example of
              the Lua language. Right Figure: an example of the HTML language.
            </p>
          </div>
        </div>
      </div>
</section> -->
<!-- <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Examples of Multilingual Completion</h2>
          <div class="content has-text-justified">
            <div class="columns is-centered">
              <img src="./images/11_bench_fim_cases.png" alt="HumanEval Overfitting" class="teaser-image center" width="80%"
                height="80%" />
            </div>
            <p>
              The data mainly consists of an instruction part
              (including an incomplete function ), a reference complete code solution and test cases. Left Figure: an
              span completion example of the C++ language. Middle Figure: a single-line completion example of
              the Rust language. Right Figure: a multiple-line completion example of the Shell language.
            </p>
          </div>
        </div>
      </div>
</section> -->
<!-- <section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Optimization Details</h2>
        <div class="content has-text-justified">
          <div class="columns is-centered">
            <img src="./images/12_ts.png" alt="HumanEval Overfitting" class="teaser-image center" width="80%"
              height="80%" />
          </div>
          <p>
            All mCoder models are fine-tuned using 8 NVIDIA A800-80GB GPUs. The models are trained for
            2 epochs with a cosine scheduler, starting at a learning rate of 2e-5 and incorporating a 3% warmup
            phase. Training a model takes about 5 hours. We used AdamW (Loshchilov and Hutter, 2017) as
            the optimizer and a batch size of 512 with a sequence truncation length of 4096. We use PyTorch’s
            Fully Sharded Data Parallel (FSDP) to perform distributed training of the model, and use gradient
            checkpointing technology and gradient accumulation to save memory and achieve training with a
            larger batch size.
          </p>
        </div>
      </div>
    </div>
</section> -->
<!-- <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Analysis of Language Representations</h2>
          <div class="content has-text-justified">
            <div class="columns is-centered">
              <img src="./images/12_cluster_merge.png" alt="HumanEval Overfitting" class="teaser-image center" width="80%"
                height="80%" />
            </div>
            <p>
              As shown in the Figure, we analyzed the programming languages in the McEval from their presentation perspective.
              We used CodeBERT to extract code representations from code snippets in McEval.
              The figure clearly shows that languages with similar syntax have closely related representations.
              For example, other functional programming languages similar to CommonLisp, as well as C, C++, Java, and scripting
              languages, exhibit high grammar similarity.
            </p>
          </div>
        </div>
      </div>
</section> -->

<!--
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code></code></pre>
    </div>
  </section>
-->
  <!-- <footer class="footer">
    <div class="container">

      <div class="columns is-centered">
        <div class="column is-8">
          <div class="content">
            <p>
              Please reach out to <a href="challenging@buaa.edu.cn">challenging@buaa.edu.cn</a> for questions or
              feedback on McEval. We are also open to collaborations and suggestions for new scenarios to add to
              the benchmark. Finally, McEval provides one axis of LLM coding evaluations and we recommend the
              following leaderboards for measuring code LM ability on various coding tasks, such as
              <a href="https://livecodebench.github.io/leaderboard.html">LiveCodeBench Leaderboard</a>,
              <a href="https://evalplus.github.io/leaderboard.html">EvalPlus Leaderboard</a>,
              <a href="https://crux-eval.github.io/leaderboard.html">CruxEval Leaderboard</a>,
              <a href="https://huggingface.co/spaces/lmsys/chatbot-arena-leaderboard">Chatbot Arena Leaderboard</a>,
              <a href="https://huggingface.co/spaces/bigcode/bigcode-models-leaderboard">BigCode Models Leaderboard</a>,
              <a href="https://infi-coder.github.io/inficoder-eval/">InfiCoder-Eval</a>, and
              <a href="https://leaderboard.tabbyml.com/">TabbyML Leaderboard</a>.
            </p>
            <p>
              The source code from this website is borrowed from <a
                href="https://github.com/LiveCodeBench/livecodebench.github.io">LiveCodeBench</a>!
            </p>
          </div>
        </div>
      </div>
    </div>
  </footer> -->
</body>

</html>
